# LocalAI: Empowering Your AI Journey ‚Äì Locally and Open Source üöÄ

<p align="center">
<img width="300" src="[https://raw.githubusercontent.com/go-skynet/LocalAI/master/core/http/static/logo.png](https://www.google.com/search?q=https://raw.githubusercontent.com/go-skynet/LocalAI/master/core/http/static/logo.png)" alt="LocalAI Logo"> <br>
<br>
</p>

<p align="center">
<a href="[https://github.com/go-skynet/LocalAI/fork](https://github.com/go-skynet/LocalAI/fork)" target="_blank">
<img src="[https://img.shields.io/github/forks/go-skynet/LocalAI?style=for-the-badge](https://img.shields.io/github/forks/go-skynet/LocalAI?style=for-the-badge)" alt="LocalAI forks"/>
</a>
<a href="[https://github.com/go-skynet/LocalAI/stargazers](https://github.com/go-skynet/LocalAI/stargazers)" target="_blank">
<img src="[https://img.shields.io/github/stars/go-skynet/LocalAI?style=for-the-badge](https://img.shields.io/github/stars/go-skynet/LocalAI?style=for-the-badge)" alt="LocalAI stars"/>
</a>
<a href="[https://github.com/go-skynet/LocalAI/pulls](https://github.com/go-skynet/LocalAI/pulls)" target="_blank">
<img src="[https://img.shields.io/github/issues-pr/go-skynet/LocalAI?style=for-the-badge](https://img.shields.io/github/issues-pr/go-skynet/LocalAI?style=for-the-badge)" alt="LocalAI pull-requests"/>
</a>
<a href='[https://github.com/go-skynet/LocalAI/releases](https://github.com/go-skynet/LocalAI/releases)'>
<img src='[https://img.shields.io/github/release/go-skynet/LocalAI?&label=Latest&style=for-the-badge](https://img.shields.io/github/release/go-skynet/LocalAI?&label=Latest&style=for-the-badge)'>
</a>
</p>

<p align="center">
<a href="[https://hub.docker.com/r/localai/localai](https://hub.docker.com/r/localai/localai)" target="_blank">
<img src="[https://img.shields.io/badge/dockerhub-images-important.svg?logo=Docker](https://img.shields.io/badge/dockerhub-images-important.svg?logo=Docker)" alt="LocalAI Docker hub"/>
</a>
<a href="[https://quay.io/repository/go-skynet/local-ai?tab=tags&tag=latest](https://quay.io/repository/go-skynet/local-ai?tab=tags&tag=latest)" target="_blank">
<img src="[https://img.shields.io/badge/quay.io-images-important.svg](https://img.shields.io/badge/quay.io-images-important.svg)?" alt="LocalAI Quay.io"/>
</a>
</p>

<p align="center">
<a href="[https://twitter.com/LocalAI_API](https://twitter.com/LocalAI_API)" target="_blank">
<img src="[https://img.shields.io/badge/X-%23000000.svg?style=for-the-badge&logo=X&logoColor=white&label=LocalAI_API](https://img.shields.io/badge/X-%23000000.svg?style=for-the-badge&logo=X&logoColor=white&label=LocalAI_API)" alt="Follow LocalAI_API"/>
</a>
<a href="[https://discord.gg/uJAeKSAGDy](https://discord.gg/uJAeKSAGDy)" target="_blank">
<img src="[https://dcbadge.vercel.app/api/server/uJAeKSAGDy?style=flat-square&theme=default-inverted](https://dcbadge.vercel.app/api/server/uJAeKSAGDy?style=flat-square&theme=default-inverted)" alt="Join LocalAI Discord Community"/>
</a>
</p>

<p align="center">
<a href="[https://trendshift.io/repositories/5539](https://trendshift.io/repositories/5539)" target="_blank"><img src="[https://trendshift.io/api/badge/repositories/5539](https://trendshift.io/api/badge/repositories/5539)" alt="mudler%2FLocalAI | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>
</p>

> :bulb: **Get Assistance!** Find answers in our [‚ùìFAQ](https://localai.io/faq/), engage in [üí≠Discussions](https://github.com/go-skynet/LocalAI/discussions), connect on [:speech_balloon: Discord](https://discord.gg/uJAeKSAGDy), or delve into our comprehensive [:book: Documentation website](https://localai.io/).
>
> **Quick Links:** [üíª Quickstart Guide](https://localai.io/basics/getting_started/) | [üñºÔ∏è Model Gallery](https://models.localai.io/) | [üöÄ Roadmap](https://github.com/mudler/LocalAI/issues?q=is%3Aissue+is%3Aopen+label%3Aroadmap) | [ü•Ω Live Demo](https://demo.localai.io) | [üåç Model Explorer](https://explorer.localai.io) | [üõ´ Practical Examples](https://github.com/mudler/LocalAI-examples) | Try it on [](https://www.google.com/search?q=%5Bhttps://t.me/localaiofficial_bot%5D(https://t.me/localaiofficial_bot))

-----

[](https://github.com/go-skynet/LocalAI/actions/workflows/test.yml)
[](https://github.com/go-skynet/LocalAI/actions/workflows/release.yaml)
[](https://github.com/go-skynet/LocalAI/actions/workflows/image.yml)
[](https://github.com/go-skynet/LocalAI/actions/workflows/bump_deps.yaml)
[](https://artifacthub.io/packages/search?repo=localai)

**LocalAI** stands as the **free, open-source alternative to OpenAI**, providing a powerful and versatile REST API for local AI inferencing. Designed to be a drop-in replacement compatible with OpenAI, Elevenlabs, and Anthropic API specifications, LocalAI empowers you to run a diverse range of **Large Language Models (LLMs)**, **generate images**, and **create audio** right on your own machine or on-premise infrastructure. What makes LocalAI truly remarkable is its ability to operate effectively with **consumer-grade hardware**, eliminating the need for a dedicated GPU for many tasks. This innovative project is proudly created and maintained by [Ettore Di Giacinto](https://github.com/mudler).

-----

## üìöüÜï Introducing the Local Stack Family: A Unified AI Ecosystem

LocalAI is no longer a standalone solution; it's now a core component of a comprehensive suite of interconnected AI tools, meticulously designed to work together seamlessly and amplify your AI capabilities:

| Project | Logo | Description |
|---|---|---|
| **[LocalAGI](https://github.com/mudler/LocalAGI)** | <a href="https://github.com/mudler/LocalAGI"><img src="https://raw.githubusercontent.com/mudler/LocalAGI/refs/heads/main/webui/react-ui/public/logo_2.png" width="200" alt="LocalAGI Logo"></a> | A robust Local AI agent management platform that functions as a drop-in replacement for OpenAI's Responses API, enhanced with advanced agentic capabilities for sophisticated AI workflows. |
| **[LocalRecall](https://github.com/mudler/LocalRecall)** | <a href="[https://github.com/mudler/LocalRecall](https://github.com/mudler/LocalRecall)"><img src="[https://raw.githubusercontent.com/mudler/LocalRecall/refs/heads/main/static/localrecall_horizontal.png](https://raw.githubusercontent.com/mudler/LocalRecall/refs/heads/main/static/localrecall_horizontal.png)" width="200" alt="LocalRecall Logo"></a> | A powerful REST-ful API and knowledge base management system that provides persistent memory and storage capabilities, offering a crucial long-term memory solution for your AI agents. |

-----

## üì∏ Visual Showcase: Explore LocalAI in Action

Witness the intuitive interfaces and powerful functionalities of LocalAI through these engaging screenshots:

| Talk Interface | Generate Audio |
| --- | --- |
|  |  |

| Models Overview | Generate Images |
| --- | --- |
|  |  |

| Chat Interface | Home |
| --- | --- |
|  |  |

| Login | Swarm |
| --- | --- |
|  |  |

-----

## üíª Get Started Quickly: Your Journey with LocalAI

Launching LocalAI is straightforward, whether you prefer an installer script or Docker containers.

### Effortless Installation

For a basic setup, simply run our convenient installer script:

```bash
# Basic installation for quick deployment
curl https://localai.io/install.sh | sh
```

Looking for more tailored installation procedures? Explore our [Installer Options](https://localai.io/docs/advanced/installer/) for advanced configurations.

### Docker Deployments: Versatility at Your Fingertips

LocalAI provides a variety of Docker images optimized for different hardware configurations:

#### **CPU-Only Image:**

Ideal for environments without dedicated GPUs.

```bash
docker run -ti --name local-ai -p 8080:8080 localai/localai:latest
```

#### **NVIDIA GPU Images:**

Leverage the power of NVIDIA GPUs for accelerated inferencing.

  * **CUDA 12.0:**
    ```bash
    docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-gpu-nvidia-cuda-12
    ```
  * **CUDA 11.7:**
    ```bash
    docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-gpu-nvidia-cuda-11
    ```
  * **NVIDIA Jetson (L4T) ARM64:**
    Optimized for NVIDIA Jetson platforms.
    ```bash
    docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-nvidia-l4t-arm64
    ```

#### **AMD GPU Images (ROCm):**

For systems equipped with AMD GPUs utilizing ROCm.

```bash
docker run -ti --name local-ai -p 8080:8080 --device=/dev/kfd --device=/dev/dri --group-add=video localai/localai:latest-gpu-hipblas
```

#### **Intel GPU Images (oneAPI):**

Harness the capabilities of Intel GPUs with oneAPI support.

  * **Intel GPU with FP16 support:**
    ```bash
    docker run -ti --name local-ai -p 8080:8080 --device=/dev/dri/card1 --device=/dev/dri/renderD128 localai/localai:latest-gpu-intel-f16
    ```
  * **Intel GPU with FP32 support:**
    ```bash
    docker run -ti --name local-ai -p 8080:8080 --device=/dev/dri/card1 --device=/dev/dri/renderD128 localai/localai:latest-gpu-intel-f32
    ```

#### **Vulkan GPU Images:**

Utilize Vulkan for cross-platform GPU acceleration.

```bash
docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-gpu-vulkan
```

#### **All-in-One (AIO) Images (Pre-downloaded Models):**

Get started even faster with AIO images that include pre-downloaded models.

  * **CPU version:**
    ```bash
    docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-aio-cpu
    ```
  * **NVIDIA CUDA 12 version:**
    ```bash
    docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-aio-gpu-nvidia-cuda-12
    ```
  * **NVIDIA CUDA 11 version:**
    ```bash
    docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-aio-gpu-nvidia-cuda-11
    ```
  * **Intel GPU version:**
    ```bash
    docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-aio-gpu-intel-f16
    ```
  * **AMD GPU version:**
    ```bash
    docker run -ti --name local-ai -p 8080:8080 --device=/dev/kfd --device=/dev/dri --group-add=video localai/localai:latest-aio-gpu-hipblas
    ```

For more in-depth information about our AIO images and the pre-downloaded models they contain, please consult our [Container Documentation](https://localai.io/basics/container/).

### Seamless Model Loading

LocalAI provides flexible options for loading AI models:

```bash
# Load a model directly from the LocalAI model gallery (explore available models with `local-ai models list`, in the WebUI's model tab, or by visiting https://models.localai.io)
local-ai run llama-3.2-1b-instruct:q4_k_m

# Launch LocalAI with a model directly from Hugging Face
local-ai run huggingface://TheBloke/phi-2-GGUF/phi-2.Q8_0.gguf

# Install and run a model from the Ollama OCI registry
local-ai run ollama://gemma:2b

# Run a model from a remote configuration file
local-ai run https://gist.githubusercontent.com/.../phi-2.yaml

# Install and run a model from a standard OCI registry (e.g., Docker Hub)
local-ai run oci://localai/phi-2:latest
```

For a more detailed guide on getting started and model usage, refer to our comprehensive [üíª Getting started](https://localai.io/basics/getting_started/index.html) documentation.

-----

## üì∞ Latest Innovations & Project News (as of July 2025)

Stay updated with the significant milestones and developments in the LocalAI project:

  * **June 2025: Backend Management Introduced!** A major enhancement, [backend management](https://github.com/mudler/LocalAI/pull/5607), has been added. **Important Note:** `extras` images are slated for deprecation starting with the next release. Please review [the backend management PR](https://github.com/mudler/LocalAI/pull/5607) for full details and transition guidance.
  * **May 2025: Advanced AI Capabilities!** This month saw the addition of [Audio input](https://github.com/mudler/LocalAI/pull/5466) and [Reranking](https://github.com/mudler/LocalAI/pull/5396) capabilities within the `llama.cpp` backend. We also launched a new [Realtime API](https://github.com/mudler/LocalAI/pull/5392) and expanded our model support to include Gemma, SmollVLM, and more multimodal models, all accessible from our model gallery.
  * **May 2025: Important Image Name Changes!** Please be aware of recent [image name changes](https://github.com/mudler/LocalAI/releases/tag/v2.29.0). Refer to the release notes to ensure compatibility with your existing deployments.
  * **April 2025: Rebrand & WebUI Enhancements!** LocalAI underwent a significant rebrand this month, accompanied by substantial improvements to its Web User Interface, offering a more streamlined and intuitive experience.
  * **April 2025: LocalAGI & LocalRecall Join the Family!** We're thrilled to announce that [LocalAGI](https://github.com/mudler/LocalAGI) and [LocalRecall](https://github.com/mudler/LocalRecall) are now officially part of the LocalAI family stack, creating a more powerful and integrated AI ecosystem.
  * **April 2025: WebUI Overhaul & AIO Image Updates!** Beyond the rebrand, the WebUI received a complete overhaul, enhancing usability and aesthetics. Our AIO (All-in-One) images were also updated for improved performance and content.
  * **February 2025: Backend Refinements & New Additions!** This month brought a backend cleanup, some breaking changes (please consult documentation for migration), and the integration of exciting new backends including `kokoro`, `OutelTTS`, and `faster-whisper`. Additionally, we introduced Nvidia L4T images for Jetson platforms.
  * **January 2025: New LocalAI Model Release & SANA Support!** We're proud to announce a new LocalAI model release: [https://huggingface.co/mudler/LocalAI-functioncall-phi-4-v0.3](https://huggingface.co/mudler/LocalAI-functioncall-phi-4-v0.3). Furthermore, SANA support was added to diffusers: [https://github.com/mudler/LocalAI/pull/4603](https://github.com/mudler/LocalAI/pull/4603).
  * **December 2024: `stablediffusion.cpp` Backend Integration!** The `stablediffusion.cpp` backend (ggml) has been successfully added to LocalAI ([https://github.com/mudler/LocalAI/pull/4289](https://github.com/mudler/LocalAI/pull/4289)), expanding our image generation capabilities.
  * **November 2024: `Bark.cpp` Backend Added!** We've integrated the `Bark.cpp` backend ([https://github.com/mudler/LocalAI/pull/4287](https://github.com/mudler/LocalAI/pull/4287)), enhancing our audio generation features.
  * **November 2024: Voice Activity Detection (VAD) API!** Voice activity detection models (**VAD**) have been added to the API, improving audio processing capabilities: [https://github.com/mudler/LocalAI/pull/4204](https://github.com/mudler/LocalAI/pull/4204).
  * **October 2024: Examples Relocated!** Our practical examples have been moved to a dedicated repository: [LocalAI-examples](https://github.com/mudler/LocalAI-examples).
  * **August 2024: FLUX-1 & P2P Explorer Launch!** We proudly introduced üÜï FLUX-1 and the new [P2P Explorer](https://explorer.localai.io), expanding our distributed AI capabilities.
  * **July 2024: üî•üî• P2P Revolution: Federated Mode & AI Swarms!** A groundbreaking release bringing a üÜï P2P Dashboard, LocalAI Federated mode, and AI Swarms: [https://github.com/mudler/LocalAI/pull/2723](https://github.com/mudler/LocalAI/pull/2723). Explore the potential of P2P Global community pools: [https://github.com/mudler/LocalAI/issues/3113](https://github.com/mudler/LocalAI/issues/3113).
  * **May 2024: üî•üî• Decentralized P2P `llama.cpp`!** Witness the power of peer-to-peer `llama.cpp` with this significant update: [https://github.com/mudler/LocalAI/pull/2343](https://github.com/mudler/LocalAI/pull/2343). Dive deeper into the documentation: [https://localai.io/features/distribute/](https://localai.io/features/distribute/).
  * **May 2024: üî•üî• Distributed Inferencing!** We've rolled out support for distributed inferencing: [https://github.com/mudler/LocalAI/pull/2324](https://github.com/mudler/LocalAI/pull/2324), enabling more scalable AI deployments.
  * **April 2024: Reranker API Released!** The Reranker API has been introduced: [https://github.com/mudler/LocalAI/pull/2121](https://github.com/mudler/LocalAI/pull/2121), enhancing retrieval capabilities.

For a forward-looking perspective on our development, view the complete list of [Roadmap items](https://github.com/mudler/LocalAI/issues?q=is%3Aissue+is%3Aopen+label%3Aroadmap).

-----

## üöÄ Powerful Features at Your Command

LocalAI is packed with features designed to make local AI inferencing accessible and powerful:

  * **üß© Backend Gallery:** Discover, install, and remove various backends dynamically. Powered by OCI images, this feature is fully customizable and entirely API-driven.
  * **üìñ Text Generation with GPTs:** Generate compelling text using a wide array of models, including those supported by `llama.cpp`, `transformers`, `vllm`, and [many more](https://localai.io/model-compatibility/index.html#model-compatibility-table).
  * **üó£ Text to Audio:** Transform written text into natural-sounding speech, perfect for voice applications and accessibility.
  * **üîà Audio to Text:** Accurately transcribe audio into text using robust models like `whisper.cpp`, enabling voice command and transcription services.
  * **üé® Image Generation:** Unleash your creativity by generating high-quality images from text prompts.
  * **üî• OpenAI-alike Tools API:** Integrate seamlessly with existing OpenAI-compatible tool specifications, allowing for familiar development workflows.
  * **üß† Embeddings Generation for Vector Databases:** Create powerful embeddings to build efficient vector databases for semantic search and retrieval.
  * **‚úçÔ∏è Constrained Grammars:** Exercise fine-grained control over model output with constrained grammars, ensuring desired formats and content.
  * **üñºÔ∏è Direct Model Downloads from Hugging Face:** Conveniently download and utilize models directly from the vast Hugging Face repository within LocalAI.
  * **ü•Ω Vision API:** Tap into advanced **GPT-Vision** capabilities for processing and understanding visual information.
  * **üìà Reranker API:** Enhance the relevance and quality of search results with our sophisticated **Reranker API**.
  * **üÜïüñß P2P Inferencing:** Embrace decentralized AI with **Peer-to-Peer (P2P) Inferencing**, allowing distributed computation across a network.
  * **Agentic Capabilities:** Build intelligent and autonomous AI agents, further empowered by integration with [LocalAGI](https://github.com/mudler/LocalAGI).
  * **üîä Voice Activity Detection (VAD):** Benefit from integrated **Silero-VAD support** for precise detection of speech segments in audio.
  * **üåç Integrated WebUI!** Enjoy a user-friendly, built-in web interface for easy management and interaction.

-----

### üîó Community & Powerful Integrations

LocalAI thrives on its vibrant community and offers extensive integration possibilities:

  * **Custom Container Development:**
      * [https://github.com/sozercan/aikit](https://github.com/sozercan/aikit)
  * **Web User Interfaces:**
      * [https://github.com/Jirubizu/localai-admin](https://github.com/Jirubizu/localai-admin)
      * [https://github.com/go-skynet/LocalAI-frontend](https://github.com/go-skynet/LocalAI-frontend)
      * **QA-Pilot:** An interactive chat project leveraging LocalAI LLMs for rapid understanding and navigation of GitHub code repositories: [https://github.com/reid41/QA-Pilot](https://github.com/reid41/QA-Pilot)
  * **Model Galleries:**
      * [https://github.com/go-skynet/model-gallery](https://github.com/go-skynet/model-gallery)
  * **Various Tools & Integrations:**
      * **Helm Chart:** Deploy LocalAI easily within Kubernetes clusters: [https://github.com/go-skynet/helm-charts](https://github.com/go-skynet/helm-charts)
      * **VSCode Extension:** Integrate LocalAI directly into your development environment: [https://github.com/badgooooor/localai-vscode-plugin](https://github.com/badgooooor/localai-vscode-plugin)
      * **Langchain:** Seamlessly integrate LocalAI with Langchain for building powerful AI applications: [https://python.langchain.com/docs/integrations/providers/localai/](https://python.langchain.com/docs/integrations/providers/localai/)
      * **Terminal Utility:** Interact with LocalAI directly from your command line: [https://github.com/djcopley/ShellOracle](https://github.com/djcopley/ShellOracle)
      * **Local Smart Assistant:** Develop intelligent local assistants: [https://github.com/mudler/LocalAGI](https://github.com/mudler/LocalAGI)
      * **Home Assistant Integrations:**
          * [https://github.com/sammcj/homeassistant-localai](https://github.com/sammcj/homeassistant-localai)
          * [https://github.com/drndos/hass-openai-custom-conversation](https://github.com/drndos/hass-openai-custom-conversation)
          * [https://github.com/valentinfrlch/ha-gpt4vision](https://github.com/valentinfrlch/ha-gpt4vision)
      * **Discord Bot:** Create engaging Discord bots powered by LocalAI: [https://github.com/mudler/LocalAGI/tree/main/examples/discord](https://github.com/mudler/LocalAGI/tree/main/examples/discord)
      * **Slack Bot:** Integrate LocalAI into your Slack workspace: [https://github.com/mudler/LocalAGI/tree/main/examples/slack](https://github.com/mudler/LocalAGI/tree/main/examples/slack)
      * **Shell-Pilot:** Interact with LLMs using LocalAI models via pure shell scripts on Linux or MacOS: [https://github.com/reid41/shell-pilot](https://github.com/reid41/shell-pilot)
      * **Telegram Bots:**
          * Official example: [https://github.com/mudler/LocalAI/tree/master/examples/telegram-bot](https://github.com/mudler/LocalAI/tree/master/examples/telegram-bot)
          * Another community bot: [https://github.com/JackBekket/Hellper](https://github.com/JackBekket/Hellper)
      * **Auto-documentation:** Automate your documentation generation: [https://github.com/JackBekket/Reflexia](https://github.com/JackBekket/Reflexia)
      * **Github Bot:** A GitHub bot that answers issues using code and documentation as context: [https://github.com/JackBekket/GitHelper](https://github.com/JackBekket/GitHelper)
      * **Github Actions:** Incorporate LocalAI into your CI/CD workflows: [https://github.com/marketplace/actions/start-localai](https://github.com/marketplace/actions/start-localai)
      * **General Examples:** Explore diverse use cases and implementation examples: [https://github.com/mudler/LocalAI/tree/master/examples/](https://github.com/mudler/LocalAI/tree/master/examples/)

-----

### üîó Essential Resources

Access valuable guides and information to enhance your LocalAI experience:

  * **[LLM Finetuning Guide](https://localai.io/docs/advanced/fine-tuning/)**: Learn how to fine-tune Large Language Models for your specific needs.
  * **[How to Build Locally](https://localai.io/basics/build/index.html)**: Step-by-step instructions for building LocalAI from source on your local machine.
  * **[How to Install in Kubernetes](https://localai.io/basics/getting_started/index.html#run-localai-in-kubernetes)**: Deploy LocalAI seamlessly within a Kubernetes environment.
  * **[Projects Integrating LocalAI](https://localai.io/docs/integrations/)**: Discover a growing list of projects and applications that utilize LocalAI.
  * **[How-Tos Section](https://io.midori-ai.xyz/howtos/)**: A curated collection of practical guides and tutorials from our community.

-----

## :book: üé• In the Spotlight: Media, Blogs, & Social Coverage

Explore how LocalAI is making waves in the AI community:

  * **[Run Visual Studio Code with LocalAI (SUSE)](https://www.suse.com/c/running-ai-locally/)**: A guide on integrating LocalAI with your VS Code environment for local AI development.
  * üÜï **[Run LocalAI on Jetson Nano Devkit](https://mudler.pm/posts/local-ai-jetson-nano-devkit/)**: A comprehensive post detailing the steps to run LocalAI on the compact and powerful NVIDIA Jetson Nano Devkit.
  * **[Run LocalAI on AWS EKS with Pulumi](https://www.pulumi.com/blog/low-code-llm-apps-with-local-ai-flowise-and-pulumi/)**: Learn how to deploy LocalAI on Amazon Web Services' Elastic Kubernetes Service using Pulumi for low-code LLM applications.
  * **[Run LocalAI on AWS](https://staleks.hashnode.dev/installing-localai-on-aws-ec2-instance)**: A guide to installing LocalAI on an AWS EC2 instance, making it accessible on the cloud.
  * **[Create a Slackbot for Teams and OSS Projects that Answer to Documentation](https://mudler.pm/posts/smart-slackbot-for-teams/)**: Discover how to build an intelligent Slackbot powered by LocalAI to efficiently answer documentation-related queries within your teams.
  * **[LocalAI Meets k8sgpt](https://www.youtube.com/watch?v=PKrDNuJ_dfE)**: An exploration of the synergy between LocalAI and k8sgpt, a tool for explaining Kubernetes resource issues.
  * **[Question Answering on Documents Locally with LangChain, LocalAI, Chroma, and GPT4All](https://mudler.pm/posts/localai-question-answering/)**: A tutorial demonstrating how to set up a local question-answering system using a powerful stack including LangChain, LocalAI, Chroma, and GPT4All.
  * **[Tutorial to Use k8sgpt with LocalAI](https://medium.com/@tyler_97636/k8sgpt-localai-unlock-kubernetes-superpowers-for-free-584790de9b65)**: A detailed guide to unlock the full potential of Kubernetes by combining k8sgpt with LocalAI, all for free.

-----

## Citation

If LocalAI has been instrumental in your research or project, please consider citing it using the following BibTeX entry:

```
@misc{localai,
  author = {Ettore Di Giacinto},
  title = {LocalAI: The free, Open source OpenAI alternative},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {url{https://github.com/go-skynet/LocalAI}},
}
```

-----

## ‚ù§Ô∏è Our Valued Sponsors

> Do you find LocalAI beneficial for your work or projects?

We invite you to support the continued development and maintenance of LocalAI by becoming a [backer or sponsor](https://github.com/sponsors/mudler). Your contribution directly helps us cover essential costs, such as CI expenses. Your logo will be proudly displayed here with a direct link to your website.

A heartfelt thank you to our generous sponsors who make this project possible:

<p align="center">
<a href="https://www.spectrocloud.com/" target="_blank">
<img height="200" src="https://github.com/user-attachments/assets/72eab1dd-8b93-4fc0-9ade-84db49f24962">
</a>
<a href="https://www.premai.io/" target="_blank">
<img height="200" src="https://github.com/mudler/LocalAI/assets/2420543/42e4ca83-661e-4f79-8e46-ae43689683d6"> <br>
</a>
</p>

-----

## üåü Star History

Track the growth and popularity of LocalAI over time:

[](https://star-history.com/#go-skynet/LocalAI&Date)

-----

## üìñ License

LocalAI is a robust, community-driven open-source project founded by [Ettore Di Giacinto](https://github.com/mudler/).

It is distributed under the **MIT License**.
Author: Ettore Di Giacinto [mudler@localai.io](mailto:mudler@localai.io)

-----

## üôá Special Acknowledgements

LocalAI's development has been greatly enriched by the foundational work and contributions of other outstanding open-source projects. We extend our sincere gratitude to:

  * [**llama.cpp**](https://github.com/ggerganov/llama.cpp): A cornerstone for efficient LLM inference.
  * [**https://github.com/tatsu-lab/stanford_alpaca**](https://github.com/tatsu-lab/stanford_alpaca)
  * [**https://github.com/cornelk/llama-go**](https://github.com/cornelk/llama-go): For providing the initial inspiration and ideas.
  * [**https://github.com/antimatter15/alpaca.cpp**](https://github.com/antimatter15/alpaca.cpp)
  * [**https://github.com/EdVince/Stable-Diffusion-NCNN**](https://github.com/EdVince/Stable-Diffusion-NCNN)
  * [**https://github.com/ggerganov/whisper.cpp**](https://github.com/ggerganov/whisper.cpp): Critical for our audio transcription capabilities.
  * [**https://github.com/rhasspy/piper**](https://github.com/rhasspy/piper)

-----

## ü§ó Our Valued Contributors

LocalAI is a true community effort, and we are incredibly grateful for the dedication and hard work of all our contributors. A special thank you to everyone who has helped shape this project!

<a href="[https://github.com/go-skynet/LocalAI/graphs/contributors](https://github.com/go-skynet/LocalAI/graphs/contributors)">
<img src="[https://contrib.rocks/image?repo=go-skynet/LocalAI](https://contrib.rocks/image?repo=go-skynet/LocalAI)" />
</a>
