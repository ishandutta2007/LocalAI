# LocalAI: Your Open-Source Alternative to OpenAI

\<p align="center"\>
\<a href="[https://github.com/go-skynet/LocalAI/fork](https://github.com/go-skynet/LocalAI/fork)" target="\_blank"\>
\<img src="[https://img.shields.io/github/forks/go-skynet/LocalAI?style=for-the-badge](https://img.shields.io/github/forks/go-skynet/LocalAI?style=for-the-badge)" alt="LocalAI forks"/\>
\</a\>
\<a href="[https://github.com/go-skynet/LocalAI/stargazers](https://github.com/go-skynet/LocalAI/stargazers)" target="\_blank"\>
\<img src="[https://img.shields.io/github/stars/go-skynet/LocalAI?style=for-the-badge](https://img.shields.io/github/stars/go-skynet/LocalAI?style=for-the-badge)" alt="LocalAI stars"/\>
\</a\>
\<a href="[https://github.com/go-skynet/LocalAI/pulls](https://github.com/go-skynet/LocalAI/pulls)" target="\_blank"\>
\<img src="[https://img.shields.io/github/issues-pr/go-skynet/LocalAI?style=for-the-badge](https://img.shields.io/github/issues-pr/go-skynet/LocalAI?style=for-the-badge)" alt="LocalAI pull-requests"/\>
\</a\>
\<a href='[https://github.com/go-skynet/LocalAI/releases](https://github.com/go-skynet/LocalAI/releases)'\>
\<img src='[https://img.shields.io/github/release/go-skynet/LocalAI?\&label=Latest\&style=for-the-badge](https://img.shields.io/github/release/go-skynet/LocalAI?&label=Latest&style=for-the-badge)'\>
\</a\>
\</p\>

\<p align="center"\>
\<a href="[https://hub.docker.com/r/localai/localai](https://hub.docker.com/r/localai/localai)" target="\_blank"\>
\<img src="[https://img.shields.io/badge/dockerhub-images-important.svg?logo=Docker](https://img.shields.io/badge/dockerhub-images-important.svg?logo=Docker)" alt="LocalAI Docker hub"/\>
\</a\>
\<a href="[https://quay.io/repository/go-skynet/local-ai?tab=tags\&tag=latest](https://quay.io/repository/go-skynet/local-ai?tab=tags&tag=latest)" target="\_blank"\>
\<img src="[https://img.shields.io/badge/quay.io-images-important.svg](https://img.shields.io/badge/quay.io-images-important.svg)?" alt="LocalAI Quay.io"/\>
\</a\>
\</p\>

\<p align="center"\>
\<a href="[https://twitter.com/LocalAI\_API](https://twitter.com/LocalAI_API)" target="\_blank"\>
\<img src="[https://img.shields.io/badge/X-%23000000.svg?style=for-the-badge\&logo=X\&logoColor=white\&label=LocalAI\_API](https://img.shields.io/badge/X-%23000000.svg?style=for-the-badge&logo=X&logoColor=white&label=LocalAI_API)" alt="Follow LocalAI\_API"/\>
\</a\>
\<a href="[https://discord.gg/uJAeKSAGDy](https://discord.gg/uJAeKSAGDy)" target="\_blank"\>
\<img src="[https://dcbadge.vercel.app/api/server/uJAeKSAGDy?style=flat-square\&theme=default-inverted](https://dcbadge.vercel.app/api/server/uJAeKSAGDy?style=flat-square&theme=default-inverted)" alt="Join LocalAI Discord Community"/\>
\</a\>
\</p\>

\<p align="center"\>
\<a href="[https://trendshift.io/repositories/5539](https://trendshift.io/repositories/5539)" target="\_blank"\>
\<img src="[https://trendshift.io/api/badge/repositories/5539](https://trendshift.io/api/badge/repositories/5539)" alt="mudler%2FLocalAI | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/\>
\</a\>
\</p\>

> :bulb: **Need Help?** Check out our [â“FAQ](https://localai.io/faq/), join [ðŸ’­Discussions](https://github.com/go-skynet/LocalAI/discussions), connect on [:speech\_balloon: Discord](https://discord.gg/uJAeKSAGDy), or explore our [:book: Documentation website](https://localai.io/).
>
> Get started with our [ðŸ’» Quickstart Guide](https://localai.io/basics/getting_started/), discover available [ðŸ–¼ï¸ Models](https://models.localai.io/), see our [ðŸš€ Roadmap](https://github.com/mudler/LocalAI/issues?q=is%3Aissue+is%3Aopen+label%3Aroadmap), try the [ðŸ¥½ Demo](https://demo.localai.io), explore the [ðŸŒ Explorer](https://explorer.localai.io), or dive into [ðŸ›« Examples](https://github.com/mudler/LocalAI-examples). You can also try it on [](https://t.me/localaiofficial_bot).

[](https://github.com/go-skynet/LocalAI/actions/workflows/test.yml)
[](https://github.com/go-skynet/LocalAI/actions/workflows/release.yaml)
[](https://github.com/go-skynet/LocalAI/actions/workflows/image.yml)
[](https://github.com/go-skynet/LocalAI/actions/workflows/bump_deps.yaml)
[](https://artifacthub.io/packages/search?repo=localai)

**LocalAI** is a free and open-source REST API that serves as a drop-in replacement for OpenAI (and compatible with Elevenlabs, Anthropic, etc.) API specifications. It empowers you to run Large Language Models (LLMs), generate images, and produce audio locally or on-premise, even with consumer-grade hardware. LocalAI supports a wide range of model families and does not require a dedicated GPU. It is proudly created and maintained by [Ettore Di Giacinto](https://github.com/mudler).

## ðŸ“šðŸ†• The Local Stack Family

LocalAI is now an integral part of a comprehensive suite of AI tools designed for seamless integration and collaboration:

| Project | Description |
|---|---|
| [](https://github.com/mudler/LocalAGI) | **[LocalAGI](https://github.com/mudler/LocalAGI)**: A powerful Local AI agent management platform that replaces OpenAI's Responses API with enhanced agentic capabilities. |
| [](https://github.com/mudler/LocalRecall) | **[LocalRecall](https://github.com/mudler/LocalRecall)**: A REST-ful API and knowledge base management system providing persistent memory and storage for AI agents. |

## Screenshots

| Talk Interface | Generate Audio |
|---|---|
|  |  |

| Models Overview | Generate Images |
|---|---|
|  |  |

| Chat Interface | Home |
|---|---|
|  |  |

| Login | Swarm |
|---|---|
|  |  |

## ðŸ’» Quickstart

Get up and running quickly with LocalAI\!

### Basic Installation

```bash
curl https://localai.io/install.sh | sh
```

For more installation options, refer to the [Installer Options](https://localai.io/docs/advanced/installer/) documentation.

### Run with Docker

#### CPU Only Image

```bash
docker run -ti --name local-ai -p 8080:8080 localai/localai:latest
```

#### NVIDIA GPU Images

  * **CUDA 12.0:**
    ```bash
    docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-gpu-nvidia-cuda-12
    ```
  * **CUDA 11.7:**
    ```bash
    docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-gpu-nvidia-cuda-11
    ```
  * **NVIDIA Jetson (L4T) ARM64:**
    ```bash
    docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-nvidia-l4t-arm64
    ```

#### AMD GPU Images (ROCm)

```bash
docker run -ti --name local-ai -p 8080:8080 --device=/dev/kfd --device=/dev/dri --group-add=video localai/localai:latest-gpu-hipblas
```

#### Intel GPU Images (oneAPI)

  * **Intel GPU with FP16 support:**
    ```bash
    docker run -ti --name local-ai -p 8080:8080 --device=/dev/dri/card1 --device=/dev/dri/renderD128 localai/localai:latest-gpu-intel-f16
    ```
  * **Intel GPU with FP32 support:**
    ```bash
    docker run -ti --name local-ai -p 8080:8080 --device=/dev/dri/card1 --device=/dev/dri/renderD128 localai/localai:latest-gpu-intel-f32
    ```

#### Vulkan GPU Images

```bash
docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-gpu-vulkan
```

#### AIO Images (Pre-downloaded Models)

  * **CPU version:**
    ```bash
    docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-aio-cpu
    ```
  * **NVIDIA CUDA 12 version:**
    ```bash
    docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-aio-gpu-nvidia-cuda-12
    ```
  * **NVIDIA CUDA 11 version:**
    ```bash
    docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-aio-gpu-nvidia-cuda-11
    ```
  * **Intel GPU version:**
    ```bash
    docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-aio-gpu-intel-f16
    ```
  * **AMD GPU version:**
    ```bash
    docker run -ti --name local-ai -p 8080:8080 --device=/dev/kfd --device=/dev/dri --group-add=video localai/localai:latest-aio-gpu-hipblas
    ```

For more details on AIO images and pre-downloaded models, see our [Container Documentation](https://localai.io/basics/container/).

### Loading Models

```bash
# From the model gallery (see available models with `local-ai models list`, in the WebUI from the model tab, or visiting https://models.localai.io)
local-ai run llama-3.2-1b-instruct:q4_k_m
# Start LocalAI with the phi-2 model directly from huggingface
local-ai run huggingface://TheBloke/phi-2-GGUF/phi-2.Q8_0.gguf
# Install and run a model from the Ollama OCI registry
local-ai run ollama://gemma:2b
# Run a model from a configuration file
local-ai run https://gist.githubusercontent.com/.../phi-2.yaml
# Install and run a model from a standard OCI registry (e.g., Docker Hub)
local-ai run oci://localai/phi-2:latest
```

For more information, refer to the [ðŸ’» Getting started](https://localai.io/basics/getting_started/index.html) guide.

## ðŸ“° Latest Project News

  * **June 2025:** [Backend management](https://github.com/mudler/LocalAI/pull/5607) has been implemented. **Attention:** `extras` images will be deprecated in the next release\! Please read [the backend management PR](https://github.com/mudler/LocalAI/pull/5607) for details.
  * **May 2025:** Exciting updates include [Audio input](https://github.com/mudler/LocalAI/pull/5466) and [Reranking](https://github.com/mudler/LocalAI/pull/5396) in the `llama.cpp` backend, a new [Realtime API](https://github.com/mudler/LocalAI/pull/5392), and expanded support for Gemma, SmollVLM, and other multimodal models (available in the gallery).
  * **May 2025:** **Important:** Image name changes have been introduced. Refer to the [release notes](https://github.com/mudler/LocalAI/releases/tag/v2.29.0).
  * **April 2025:** Rebranding and significant WebUI enhancements.
  * **April 2025:** [LocalAGI](https://github.com/mudler/LocalAGI) and [LocalRecall](https://github.com/mudler/LocalRecall) officially join the LocalAI family stack.
  * **April 2025:** Comprehensive WebUI overhaul and AIO images updates.
  * **February 2025:** Backend cleanup, breaking changes, and new backends (kokoro, OutelTTS, faster-whisper), along with Nvidia L4T images.
  * **January 2025:** LocalAI model release: [https://huggingface.co/mudler/LocalAI-functioncall-phi-4-v0.3](https://huggingface.co/mudler/LocalAI-functioncall-phi-4-v0.3), SANA support in diffusers: [https://github.com/mudler/LocalAI/pull/4603](https://github.com/mudler/LocalAI/pull/4603).
  * **December 2024:** `stablediffusion.cpp` backend (ggml) added ([https://github.com/mudler/LocalAI/pull/4289](https://github.com/mudler/LocalAI/pull/4289)).
  * **November 2024:** `Bark.cpp` backend added ([https://github.com/mudler/LocalAI/pull/4287](https://github.com/mudler/LocalAI/pull/4287)).
  * **November 2024:** Voice activity detection models (**VAD**) added to the API: [https://github.com/mudler/LocalAI/pull/4204](https://github.com/mudler/LocalAI/pull/4204).
  * **October 2024:** Examples have been moved to [LocalAI-examples](https://github.com/mudler/LocalAI-examples).
  * **August 2024:** ðŸ†• FLUX-1 and the [P2P Explorer](https://explorer.localai.io) are now available.
  * **July 2024:** ðŸ”¥ðŸ”¥ ðŸ†• P2P Dashboard, LocalAI Federated mode, and AI Swarms introduced: [https://github.com/mudler/LocalAI/pull/2723](https://github.com/mudler/LocalAI/pull/2723). Explore P2P Global community pools: [https://github.com/mudler/LocalAI/issues/3113](https://github.com/mudler/LocalAI/issues/3113).
  * **May 2024:** ðŸ”¥ðŸ”¥ Decentralized P2P `llama.cpp` is here: [https://github.com/mudler/LocalAI/pull/2343](https://github.com/mudler/LocalAI/pull/2343) (peer2peer `llama.cpp`\!). ðŸ‘‰ Docs: [https://localai.io/features/distribute/](https://localai.io/features/distribute/).
  * **May 2024:** ðŸ”¥ðŸ”¥ Distributed inferencing: [https://github.com/mudler/LocalAI/pull/2324](https://github.com/mudler/LocalAI/pull/2324).
  * **April 2024:** Reranker API: [https://github.com/mudler/LocalAI/pull/2121](https://github.com/mudler/LocalAI/pull/2121).

For a complete list of upcoming features, check our [Roadmap items](https://github.com/mudler/LocalAI/issues?q=is%3Aissue+is%3Aopen+label%3Aroadmap).

## ðŸš€ [Key Features](https://localai.io/features/)

  * **ðŸ§© Backend Gallery:** Install and remove backends on the fly, powered by OCI images â€“ fully customizable and API-driven.
  * **ðŸ“– Text Generation with GPTs:** Supports `llama.cpp`, `transformers`, `vllm`, and [many more](https://localai.io/model-compatibility/index.html#model-compatibility-table).
  * **ðŸ—£ Text to Audio:** Convert text into natural-sounding speech.
  * **ðŸ”ˆ Audio to Text:** Perform audio transcription with `whisper.cpp`.
  * **ðŸŽ¨ Image Generation:** Create images using various models.
  * **ðŸ”¥ OpenAI-alike Tools API:** Leverage an API compatible with OpenAI's tool specifications.
  * **ðŸ§  Embeddings Generation:** Generate embeddings for vector databases.
  * **âœï¸ Constrained Grammars:** Control output generation with constrained grammars.
  * **ðŸ–¼ï¸ Download Models Directly from Hugging Face:** Easily access and use models from the Hugging Face hub.
  * **ðŸ¥½ Vision API:** Integrate GPT-Vision capabilities.
  * **ðŸ“ˆ Reranker API:** Enhance search and retrieval with reranking.
  * **ðŸ†•ðŸ–§ P2P Inferencing:** Utilize distributed inferencing capabilities.
  * **Agentic Capabilities:** Build intelligent agents with [LocalAGI](https://github.com/mudler/LocalAGI).
  * **ðŸ”Š Voice Activity Detection:** Includes Silero-VAD support.
  * **ðŸŒ Integrated WebUI\!**

### ðŸ”— Community and Integrations

LocalAI fosters a vibrant community and integrates with numerous projects:

  * **Build and deploy custom containers:**
      * [https://github.com/sozercan/aikit](https://github.com/sozercan/aikit)
  * **WebUIs:**
      * [https://github.com/Jirubizu/localai-admin](https://github.com/Jirubizu/localai-admin)
      * [https://github.com/go-skynet/LocalAI-frontend](https://github.com/go-skynet/LocalAI-frontend)
      * QA-Pilot (An interactive chat project that leverages LocalAI LLMs for rapid understanding and navigation of GitHub code repositories) [https://github.com/reid41/QA-Pilot](https://github.com/reid41/QA-Pilot)
  * **Model galleries:**
      * [https://github.com/go-skynet/model-gallery](https://github.com/go-skynet/model-gallery)
  * **Other:**
      * Helm chart [https://github.com/go-skynet/helm-charts](https://github.com/go-skynet/helm-charts)
      * VSCode extension [https://github.com/badgooooor/localai-vscode-plugin](https://github.com/badgooooor/localai-vscode-plugin)
      * Langchain: [https://python.langchain.com/docs/integrations/providers/localai/](https://python.langchain.com/docs/integrations/providers/localai/)
      * Terminal utility [https://github.com/djcopley/ShellOracle](https://github.com/djcopley/ShellOracle)
      * Local Smart assistant [https://github.com/mudler/LocalAGI](https://github.com/mudler/LocalAGI)
      * Home Assistant [https://github.com/sammcj/homeassistant-localai](https://github.com/sammcj/homeassistant-localai) / [https://github.com/drndos/hass-openai-custom-conversation](https://github.com/drndos/hass-openai-custom-conversation) / [https://github.com/valentinfrlch/ha-gpt4vision](https://github.com/valentinfrlch/ha-gpt4vision)
      * Discord bot [https://github.com/mudler/LocalAGI/tree/main/examples/discord](https://github.com/mudler/LocalAGI/tree/main/examples/discord)
      * Slack bot [https://github.com/mudler/LocalAGI/tree/main/examples/slack](https://github.com/mudler/LocalAGI/tree/main/examples/slack)
      * Shell-Pilot (Interact with LLM using LocalAI models via pure shell scripts on your Linux or MacOS system) [https://github.com/reid41/shell-pilot](https://github.com/reid41/shell-pilot)
      * Telegram bot [https://github.com/mudler/LocalAI/tree/master/examples/telegram-bot](https://github.com/mudler/LocalAI/tree/master/examples/telegram-bot)
      * Another Telegram Bot [https://github.com/JackBekket/Hellper](https://github.com/JackBekket/Hellper)
      * Auto-documentation [https://github.com/JackBekket/Reflexia](https://github.com/JackBekket/Reflexia)
      * Github bot which answers on issues, with code and documentation as context [https://github.com/JackBekket/GitHelper](https://github.com/JackBekket/GitHelper)
      * Github Actions: [https://github.com/marketplace/actions/start-localai](https://github.com/marketplace/actions/start-localai)
      * Examples: [https://github.com/mudler/LocalAI/tree/master/examples/](https://github.com/mudler/LocalAI/tree/master/examples/)

### ðŸ”— Resources

  * [LLM finetuning guide](https://localai.io/docs/advanced/fine-tuning/)
  * [How to build locally](https://localai.io/basics/build/index.html)
  * [How to install in Kubernetes](https://localai.io/basics/getting_started/index.html#run-localai-in-kubernetes)
  * [Projects integrating LocalAI](https://localai.io/docs/integrations/)
  * [How-to section](https://io.midori-ai.xyz/howtos/) (curated by our community)

## :book: ðŸŽ¥ [Media, Blogs, Social](https://localai.io/basics/news/#media-blogs-social)

  * [Run Visual Studio Code with LocalAI (SUSE)](https://www.suse.com/c/running-ai-locally/)
  * ðŸ†• [Run LocalAI on Jetson Nano Devkit](https://mudler.pm/posts/local-ai-jetson-nano-devkit/)
  * [Run LocalAI on AWS EKS with Pulumi](https://www.pulumi.com/blog/low-code-llm-apps-with-local-ai-flowise-and-pulumi/)
  * [Run LocalAI on AWS](https://staleks.hashnode.dev/installing-localai-on-aws-ec2-instance)
  * [Create a Slackbot for teams and OSS projects that answer to documentation](https://mudler.pm/posts/smart-slackbot-for-teams/)
  * [LocalAI meets k8sgpt](https://www.youtube.com/watch?v=PKrDNuJ_dfE)
  * [Question Answering on Documents locally with LangChain, LocalAI, Chroma, and GPT4All](https://mudler.pm/posts/localai-question-answering/)
  * [Tutorial to use k8sgpt with LocalAI](https://medium.com/@tyler_97636/k8sgpt-localai-unlock-kubernetes-superpowers-for-free-584790de9b65)

## Citation

If you utilize this repository or its data in a downstream project, please consider citing it with:

```
@misc{localai,
  author = {Ettore Di Giacinto},
  title = {LocalAI: The free, Open source OpenAI alternative},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/go-skynet/LocalAI}},
}
```

## â¤ï¸ Sponsors

> Do you find LocalAI useful?

Support the project by becoming [a backer or sponsor](https://github.com/sponsors/mudler). Your logo will be featured here with a link to your website.

A huge thank you to our generous sponsors who support this project by covering CI expenses, and our [Sponsor list](https://github.com/sponsors/mudler):

\<p align="center"\>
\<a href="https://www.spectrocloud.com/" target="\_blank"\>
\<img height="200" src="https://github.com/user-attachments/assets/72eab1dd-8b93-4fc0-9ade-84db49f24962"\>
\</a\>
\<a href="https://www.premai.io/" target="\_blank"\>
\<img height="200" src="https://github.com/mudler/LocalAI/assets/2420543/42e4ca83-661e-4f79-8e46-ae43689683d6"\> \<br\>
\</a\>
\</p\>

## ðŸŒŸ Star History

[](https://www.google.com/search?q=%5Bhttps://star-history.com/%23go-skynet/LocalAI%26Date%5D\(https://star-history.com/%23go-skynet/LocalAI%26Date\))

## ðŸ“– License

LocalAI is a community-driven project created by [Ettore Di Giacinto](https://github.com/mudler/).

MIT - Author Ettore Di Giacinto [mudler@localai.io](mailto:mudler@localai.io)

## ðŸ™‡ Acknowledgements

LocalAI couldn't have been built without the help of great software already available from the community. Thank you\!

  * [llama.cpp](https://github.com/ggerganov/llama.cpp)
  * [https://github.com/tatsu-lab/stanford\_alpaca](https://github.com/tatsu-lab/stanford_alpaca)
  * [https://github.com/cornelk/llama-go](https://github.com/cornelk/llama-go) for the initial ideas
  * [https://github.com/antimatter15/alpaca.cpp](https://github.com/antimatter15/alpaca.cpp)
  * [https://github.com/EdVince/Stable-Diffusion-NCNN](https://github.com/EdVince/Stable-Diffusion-NCNN)
  * [https://github.com/ggerganov/whisper.cpp](https://github.com/ggerganov/whisper.cpp)
  * [https://github.com/rhasspy/piper](https://github.com/rhasspy/piper)

## ðŸ¤— Contributors

This is a community project, a special thanks to our contributors\! ðŸ¤—
\<a href="[https://github.com/go-skynet/LocalAI/graphs/contributors](https://github.com/go-skynet/LocalAI/graphs/contributors)"\>
\<img src="[https://contrib.rocks/image?repo=go-skynet/LocalAI](https://contrib.rocks/image?repo=go-skynet/LocalAI)" /\>
\</a\>
